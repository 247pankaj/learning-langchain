{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever And Chain - Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content='Mitigating Hallucination in Multimodal LLMs with\\nLayer Contrastive Decoding\\nBingkui Tong1 Jiaer Xia2 Kaiyang Zhou2\\n1Mohamed bin Zayed University of Artificial Intelligence\\n2Hong Kong Baptist University\\nAbstract\\nMultimodal Large Language Models (MLLMs) have shown impressive perception\\nand reasoning capabilities, yet they often suffer from hallucinations‚Äîgenerating\\noutputs that are linguistically coherent but inconsistent with the context of the\\ninput image, including inaccuracies in objects, attributes, and relations. To address\\nthis challenge, we propose a simple approach called Layer Contrastive Decoding\\n(LayerCD). Our design is motivated by the observation that shallow visual features\\nare much more likely than deep visual features to cause an MLLM to hallucinate\\nas they only capture biased, low-level information that is insufficient for high-level\\nreasoning. Therefore, LayerCD aims to filter out hallucinations by contrasting the\\noutput distributions generated from visual features of different levels, specifically\\nthose from the shallow and deep layers of the vision encoder, respectively. We\\nconduct extensive experiments on two hallucination benchmarks and show that\\nLayerCD significantly outperforms current state-of-the-art. The code for LayerCD\\nis available at maifoundations/LayerCD.\\n1 Introduction\\nQ: What is the wall made of that the motorcycles are  \\nparked in front of ?\\nA: Wood \\nQ: How is the brick building associated with the scene?\\nA: The motorcycles are driving towards it \\nQ: How many men are lying on the motorcycles?\\nA: None\\nQ: What is the wall made of that the motorcycles are \\nparked in front of ?\\nA: Brick\\nQ: How is the brick building associated with the scene?\\nA: It is behind the motorcycles\\nQ: How many men are lying on the motorcycles?\\nA: One\\nFigure 1: Evaluation of LLaV A 1.5, a state-of-the-\\nart MLLM, using different levels of visual features.\\nThe results suggest that shallow features lead to\\nsignificantly higher hallucination error rates com-\\npared to deeper features.\\nLLMs have long struggled with hallucination,\\na phenomenon where model outputs appear\\nplausible but are factually incorrect or fabri-\\ncated Maynez et al. [2020]. Unfortunately, mul-\\ntimodal LLMs (MLLMs)‚Äîwhich incorporate\\nan additional vision module to process images‚Äî\\nalso face this issue. In MLLMs, hallucination\\noccurs when the model generates responses that\\nare fluent and coherent yet misaligned with the\\nvisual input Liu et al. [2024b]. These inconsis-\\ntencies often manifest as inaccuracies in identi-\\nfying objects, attributes, or relationships, limit-\\ning the model‚Äôs ability to accurately interpret im-\\nages and posing a significant challenge for real-\\nworld deployment and practical applications.\\nWe propose Layer Contrastive Decoding (Lay-\\nerCD), a simple, inference-time method that re-\\nquires no architectural changes. Our approach is\\nmotivated by the key observation that MLLMs\\nare more prone to hallucination when condi-\\ntioned on shallow versus deep visual features.\\nBy contrasting the output distributions from\\nMultimodal Algorithmic Reasoning Workshop at NeurIPS 2025 (MAR-NeurIPS 2025).\\narXiv:2509.25177v1  [cs.CV]  29 Sep 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}, page_content=\"‚Ä¶\\nFirst Layer\\nWhat color is \\nthe text on the \\nleft person's \\nclothes?\\nVision Encoder\\nVisual Input Textual Input\\nùê©ùêãùêöùê≤ùêûùê´ùêÇùêÉ\\nLast Layer\\nLanguage Model\\nDefault:  ['Black</s>']\\nToken: 'Black'  Score: 0.5645\\nToken: 'White'  Score: 0.3318\\nToken: 'Y'      Score: 0.0158\\nToken: 'Blue'   Score: 0.0148\\nToken: 'Gray'   Score: 0.0091\\nToken: 'Red'    Score: 0.0073\\nToken: 'Orange' Score: 0.0068\\nToken: 'Dark'   Score: 0.0054\\nToken: 'Green'  Score: 0.0054\\nToken: 'Silver' Score: 0.0047\\nFirst Layer Output:  ['Red</s>']\\nToken: 'Red'    Score: 0.1643\\nToken: 'Orange' Score: 0.1090\\nToken: 'Black'  Score: 0.0852\\nToken: 'Y'      Score: 0.0704\\nToken: 'Blue'   Score: 0.0600\\nToken: 'P'      Score: 0.0555\\nToken: 'Brown'  Score: 0.0497\\nToken: 'Mar'    Score: 0.0430\\nToken: 'Pur'    Score: 0.0386\\nToken: 'White'  Score: 0.0331\\nLayerCD Output:  ['White</s>']\\nToken: 'White'  Score: 0.5776\\nToken: 'Black'  Score: 0.4226\\nToken: '<unk>'  Score: 0.0000\\nToken: '<s>'    Score: 0.0000\\nToken: '<0x03>' Score: 0.0000\\nToken: '<0x04>' Score: 0.0000\\nToken: '<0x02>' Score: 0.0000\\nToken: '<0x01>' Score: 0.0000\\nToken: '</s>'   Score: 0.0000\\nToken: '<0x00>' Score: 0.0000\\nBlack\\nWhite\\nBlue\\nRed\\nOrange\\n‚Ä¶‚Ä¶\\n0.56\\n0.33\\n0.01\\n0.01\\n0.01\\nRed\\nOrange\\nBlack\\nBlue\\nWhite\\n‚Ä¶‚Ä¶\\n0.16\\n0.11\\n0.09\\n0.06\\n0.03\\nùê©\\nùê©‚Ä≤\\nWhite\\nBlack\\nRed\\nBlack\\nOrange\\n‚Ä¶‚Ä¶\\n0.58\\n0.41\\n0.00\\n0.00\\n0.00\\nFigure 2: Overview of Layer Contrastive Decoding (LayerCD). The main idea is to factor out\\nhallucinations by contrasting output distributions derived from different levels of visual features,\\nspecifically those from the shallow and deep layers of the vision encoder, respectively.\\nthese two feature levels, LayerCD effectively reduces hallucinations. A preliminary study using\\nLLaV A 1.5 Liu et al. [2024c] validates this; as shown in Fig. 1, using shallow features leads to\\nsignificantly higher hallucination error rates. This is because shallow features capture only low-level\\ncharacteristics like edges and colors, which are insufficient for high-level reasoning and thus more\\nlikely to cause hallucinations Zeiler and Fergus [2014], Yosinski et al. [2014].\\nFig. 2 illustrates the LayerCD mechanism and the crucial differences in the resulting output distri-\\nbutions. While a model using deep features may produce hallucinations (e.g., ‚ÄúBlack‚Äù, Blue‚Äù), it\\nstill assigns non-trivial probability to the correct token (e.g., ‚ÄúWhite‚Äù). In contrast, conditioning\\non shallow features yields more high-confidence hallucinations while suppressing the correct token.\\nDrawing inspiration from Contrastive Decoding Li et al. [2022], which was designed to reduce repeti-\\ntion in text, LayerCD contrasts these two distributions. This readjustment cancels out hallucinations\\nand allows the correct token (‚ÄúWhite‚Äù) to emerge with the highest confidence.\\nIn summary, we make the following contributions in this paper: (1) We provide an important insight\\nthat shallow visual features are more prone to causing hallucinations in MLLMs than deep visual\\nfeatures; (2) Based on the insight, we propose a simple contrastive decoding approach that contrasts\\nvisual features of different layers to factor out hallucinations; (3) We demonstrate the effectiveness of\\nour approach on two hallucination benchmarks where our approach significantly outperforms current\\nstate-of-the-art.\\n2 Methodology\\nOur proposed Layer Contrastive Decoding (LayerCD) is an inference-time, architecture-free method\\nto mitigate hallucination. It is motivated by the observation that contrasting the outputs from shallow\\nvisual features (high confidence on hallucinations, low on correct tokens) and deep features (high\\nconfidence on both) can isolate and suppress erroneous tokens. During decoding, an MLLM takes\\nvisual features z and text input x to produce the next-token probability, conditioned on previous\\ntokensy <t:p(y t|x,z,y <t)\\n2.1 Layer Contrastive Decoding\\nContrastive Probability DistributionAs discussed previously, shallow visual features are more\\nprone than deep features to cause hallucinations and therefore the two output distributions derived\\nfrom these two types of features can form a contrastive pair, which allows the model to pinpoint and\\nthen cancel out hallucinations in the output. Let zs denote image features extracted from a shallow\\nlayer (e.g., the first layer) in the vision encoder, zd image features extracted from a deep layer (e.g.,\\n2\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3'}, page_content='the last layer), andf(¬∑)the LLM, the contrastive probability distribution can be formulated as\\np(y|x,z d,z s) =œÉ[(1 +Œ±)f(x,z d)‚àíŒ±f(x,z s)],(1)\\nwhere œÉ[¬∑] is the softmax function, and Œ± is a hyperparameter that controls the amplification of the\\ndifference between the two distributions. In particular, a largerŒ± indicates a stronger contrast between\\nthe two distributions. WhenŒ±=0, the formulation returns to regular decoding.\\nSince LayerCD only modifies the next-token probability distribution, it can be easily combined with\\nexisting decoding methods, such as nucleus sampling, beam search, and others. It is worth noting that\\nthe formulation in Eq. 1 does not require modifying the internal model parameters. Unless otherwise\\nspecified, we select the first and last vision encoder layer for extractingz s andz d, respectively.\\nAdaptive Plausibility ConstraintThe formulation in Eq. 1 indiscriminately penalizes all outputs\\nfrom the shallow-feature model, an assumption that is too strong in practice. Common tokens like\\narticles (e.g., ‚Äúa‚Äù, ‚Äúthe‚Äù) are often predicted with high confidence regardless of feature depth, and\\nshould not be penalized. To prevent the generation of implausible outputs, we introduce an adaptive\\nplausibility constraint, inspired by Li et al. Li et al. [2022]. At each inference step, we dynamically\\nfilter the vocabulary by removing tokens whose confidence in the original deep-feature distribution\\nfalls below a threshold based on the maximum confidence. This ensures that LayerCD is computed\\nusing only this updated set of plausible tokens:\\nVhead(y<t) ={yt ‚àà V:p(y t|x,z d,y <t)‚â•Œ≤max\\nw\\np(w|x,z d,y <t)}, (2)\\nwhere V is the vocabulary of the MLLM andŒ≤ is a hyperparameter between 0 and 1 that controls how\\naggressively low-confidence tokens are pruned. A larger Œ≤ results in a more aggressive truncation,\\nretaining only tokens with higher confidence.1\\nBy combining the contrastive probability distribution with the adaptive plausibility constraint, the\\nt-th token is computed as\\np(yt|x,z d,z s,y <t)s.t.y t ‚àà Vhead(y<t). (3)\\n3 Experiments on POPE\\nPOPEWe evaluate hallucination using the POPE benchmark Li et al. [2023a], which contains\\nimages from MSCOCO Lin et al. [2014], A-OKVQA Schwenk et al. [2022], and GQA Hudson\\nand Manning [2019]. Models must answer yes/no questions probing for objects that are absent,\\ngenerated viarandom,popular(high-frequency), andadversarial(co-occurring) sampling. We report\\nthe average Accuracy, Precision, Recall, and F1 scores over five runs.\\nModels and BaselineTo evaluate the robustness and adaptability of our methods, we select three\\nstate-of-the-art MLLMs with diverse architectures: LLaV A-v1.5-7B Liu et al. [2024c], Cambrian-\\n8B Tong et al. [2024], and MoLmo-7B-D Deitke et al. [2025]. LLaV A-v1.5-7B utilizes a CLIP\\nViT-L/14@336 Radford et al. [2021] vision encoder and the Vicuna-v1.5-7B Zheng et al. [2023]\\nLLM. Cambrian-8B combines multiple vision encoders (e.g., CLIP ViT-L/14@336 Radford et al.\\n[2021], ConvNeXt-XXL@1024 Woo et al. [2023], DINOv2 Giant Oquab et al. [2023], and SigLIP\\nViT-SO400M/14@384 Zhai et al. [2023b]) with the LLaMA3-8B-Instruct Dubey et al. [2024] LLM.\\nWe chose MoLmo-7B-D, which pairs a CLIP ViT-L/336 Radford et al. [2021] encoder with the\\nQWen2-7B Yang et al. [2024] LLM, due to its superior performance over QWen-VL Bai et al. [2023]\\non 11 benchmarks.\\nFor baselines, we compare LayerCD with regular decoding and VCD Leng et al. [2024]. Please refer\\nto Appendix for discussions on the differences between LayerCD and VCD.\\nImplementation DetailsFor LayerCD, we set the contrastive amplification parameter Œ± to 1 and\\nthe plausibility constraint‚Äôs truncation parameter Œ≤ to 0.1. The shallow features zs and deep features\\nzd are extracted from the first and last default layer of the vision encoder, respectively. The output\\ntokens are sampled from the post-softmax layer after the decoding strategy is applied.\\n1Similar to Eq. 1, we compute this constraint using logit value instead of post-softmax probability.\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4'}, page_content='Setting Model Decoding Accuracy Precision Recall F1 Score\\nRandom\\nLLaV A1.5\\nRegular83.21 ¬±0.49 92.21¬±0.70 72.55¬±0.63 81.20¬±0.56\\nVCD82.33 ¬±0.16 95.42¬±0.52 67.93¬±0.30 79.36¬±0.17\\nLayerCD85.77 ¬±0.25 96.39¬±0.32 74.32¬±0.59 83.93¬±0.34\\nCambrian\\nRegular62.47 ¬±0.40 81.22¬±1.58 32.45¬±0.55 46.37¬±0.56\\nVCD64.59 ¬±0.56 94.54¬±1.43 30.96¬±0.82 46.64¬±1.04\\nLayerCD75.65 ¬±0.33 97.21¬±0.22 52.81¬±0.75 68.44¬±0.60\\nMolmo\\nRegular45.67 ¬±4.08 39.87¬±8.55 18.85¬±6.24 25.54¬±7.44\\nVCD50.29 ¬±0.47 50.58¬±0.96 24.80¬±1.04 33.28¬±1.14\\nLayerCD60.79 ¬±0.05 63.49¬±0.11 50.81¬±0.56 56.45¬±0.30\\nPopular\\nLLaV A1.5\\nRegular81.83 ¬±0.47 88.99¬±0.53 72.64¬±0.66 79.99¬±0.55\\nVCD80.99 ¬±0.17 91.87¬±0.57 68.01¬±0.28 78.16¬±0.16\\nLayerCD84.25 ¬±0.25 92.60¬±0.17 74.44¬±0.59 82.53¬±0.34\\nCambrian\\nRegular61.18 ¬±0.56 76.19¬±1.84 32.56¬±0.55 45.61¬±0.64\\nVCD62.91 ¬±0.68 88.19¬±1.28 29.81¬±1.18 44.55¬±1.42\\nLayerCD73.87 ¬±0.30 91.45¬±0.71 52.68¬±0.68 66.84¬±0.50\\nMolmo\\nRegular44.78 ¬±3.23 37.52¬±7.52 16.43¬±4.45 22.83¬±5.68\\nVCD49.21 ¬±0.32 48.22¬±0.77 21.47¬±0.77 29.70¬±0.89\\nLayerCD58.83 ¬±0.33 61.60¬±0.45 46.92¬±0.26 53.27¬±0.34\\nAdversarial\\nLLaV A1.5\\nRegular79.04 ¬±0.43 83.35¬±0.41 72.57¬±0.62 77.59¬±0.50\\nVCD78.32 ¬±0.35 85.72¬±0.93 67.97¬±0.33 75.82¬±0.26\\nLayerCD82.09 ¬±0.43 87.96¬±0.82 74.36¬±0.50 80.59¬±0.42\\nCambrian\\nRegular61.23 ¬±0.44 76.66¬±1.45 32.31¬±0.67 45.45¬±0.69\\nVCD62.31 ¬±0.36 84.67¬±1.71 30.12¬±1.32 44.40¬±1.29\\nLayerCD73.97 ¬±0.28 91.94¬±0.56 52.56¬±0.66 66.88¬±0.50\\nMolmo\\nRegular44.81 ¬±3.29 39.05¬±6.78 19.25¬±4.51 25.77¬±5.50\\nVCD48.43 ¬±0.35 46.87¬±0.77 23.59¬±0.96 31.38¬±1.00\\nLayerCD58.06 ¬±0.62 59.92¬±0.76 48.69¬±0.62 53.73¬±0.68\\nTable 1: Results on POPE-MSCOCO. Our LayerCD outperforms VCD and regular decoding by\\nsignificant margins. The results on POPE-A-OKVQA and POPE-GQA are provided in the supple-\\nmentary where the conclusion remains the same.\\nResults on POPETable 1 summarizes the results of applying different decoding strategies to\\ndifferent base models in the three distinct sampling settings. Overall, LayerCD achieves robust perfor-\\nmance across all sampling settings and with different base models. Compared with regular decoding\\nand VCD, LayerCD gains significant improvements consistently across all settings. These results\\nstrongly demonstrate the effectiveness of using shallow features to filter out object hallucinations\\n(POPE only focuses on object hallucinations). It is worth noting that all models have relatively weak\\nperformance in terms of recall. This is probably due to the training data bias that causes the model to\\nanswer ‚ÄúNo‚Äù. Nonetheless, LayerCD still demonstrates huge gains against the baselines.\\n4 Conclusion and Limitations\\nIn this work, we propose Layer Contrastive Decoding (LayerCD), a simple and effective approach\\nbuilt on the key observation that shallow visual features are significantly more prone to inducing\\nMLLM hallucinations than deep ones. By leveraging the contrast between these feature levels,\\nLayerCD effectively pinpoints and cancels out erroneous content.\\nA primary limitation of our approach is the doubled computational cost, as it requires separate forward\\npasses for shallow and deep features‚Äîa drawback inherited from the original contrastive decoding\\ndesign. This presents a challenge for extremely large models, and we leave the development of more\\nefficient contrastive frameworks for future work.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5'}, page_content='5 Acknowledgements\\nThis research is supported by Hong Kong Research Grants Council Early Career Scheme (No.\\n22200824).\\nReferences\\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and\\nJingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and\\nbeyond.arXiv preprint arXiv:2308.12966, 1(2):3, 2023.\\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction\\nwith recurrent neural networks.Advances in neural information processing systems, 28, 2015.\\nZhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou\\nZhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic\\ntasks. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\\n24185‚Äì24198, 2024.\\nMatt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi,\\nNiklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong\\nNgo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen\\nBastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg,\\nMichael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta,\\nKuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa\\nSchoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick,\\nAli Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art\\nvision-language models. pages 91‚Äì104, June 2025.\\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\\nAkhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.arXiv preprint\\narXiv:2407.21783, 2024.\\nAlessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro\\nAchille, Ashwin Swaminathan, and Stefano Soatto. Multi-modal hallucination control by visual information\\ngrounding. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\\n14303‚Äì14312, 2024.\\nMarkus Freitag and Yaser Al-Onaizan. Beam search strategies for neural machine translation.arXiv preprint\\narXiv:1702.01806, 2017.\\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,\\nKe Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for\\nmultimodal large language models, 2024. URLhttps://arxiv.org/abs/2306.13394.\\nUlrich Germann. Greedy decoding for statistical machine translation in almost linear time. InProceedings of\\nthe 2003 Human Language Technology Conference of the North American Chapter of the Association for\\nComputational Linguistics, pages 72‚Äì79, 2003.\\nAnisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language\\nmodels. InProceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 18135‚Äì18143,\\n2024.\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration.\\narXiv preprint arXiv:1904.09751, 2019.\\nQidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and\\nNenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty\\nand retrospection-allocation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 13418‚Äì13427, 2024.\\nDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and\\ncompositional question answering. InProceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 6700‚Äì6709, 2019.\\nJitesh Jain, Jianwei Yang, and Humphrey Shi. Vcoder: Versatile vision encoders for multimodal large language\\nmodels. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\\n27992‚Äì28002, 2024.\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6'}, page_content='Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang,\\nand Shikun Zhang. Hallucination augmented contrastive learning for multimodal large language model. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27036‚Äì27046,\\n2024.\\nR√©mi Lebret, David Grangier, and Michael Auli. Neural text generation from structured data with application to\\nthe biography domain.arXiv preprint arXiv:1603.07771, 2016.\\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and\\nNicholas Carlini. Deduplicating training data makes language models better.arXiv preprint arXiv:2107.06499,\\n2021.\\nNayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, and Bryan Catanzaro.\\nFactuality enhanced language models for open-ended text generation.Advances in Neural Information\\nProcessing Systems, 35:34586‚Äì34599, 2022.\\nSicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating\\nobject hallucinations in large vision-language models through visual contrastive decoding. InProceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13872‚Äì13882, 2024.\\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer,\\nand Mike Lewis. Contrastive decoding: Open-ended text generation as optimization.arXiv preprint\\narXiv:2210.15097, 2022.\\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucina-\\ntion in large vision-language models.arXiv preprint arXiv:2305.10355, 2023a.\\nZhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai.\\nMonkey: Image resolution and text label are important things for large multi-modal models. InProceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26763‚Äì26773, 2024.\\nZuchao Li, Shitou Zhang, Hai Zhao, Yifei Yang, and Dongjie Yang. Batgpt: A bidirectional autoregessive talker\\nfrom generative pre-trained transformer.arXiv preprint arXiv:2307.00360, 2023b.\\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods.\\narXiv preprint arXiv:2109.07958, 2021.\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and\\nC Lawrence Zitnick. Microsoft coco: Common objects in context. InComputer Vision‚ÄìECCV 2014: 13th\\nEuropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740‚Äì755.\\nSpringer, 2014.\\nBingbin Liu, Jordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Exposing attention glitches\\nwith flip-flop language modeling.Advances in Neural Information Processing Systems, 36, 2024a.\\nFuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in\\nlarge multi-modal models via robust instruction tuning. InThe Twelfth International Conference on Learning\\nRepresentations, 2023.\\nHanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and\\nWei Peng. A survey on hallucination in large vision-language models.arXiv preprint arXiv:2402.00253,\\n2024b.\\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26296‚Äì26306,\\n2024c.\\nShayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based\\nknowledge conflicts in question answering.arXiv preprint arXiv:2109.05052, 2021.\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in\\nabstractive summarization.arXiv preprint arXiv:2005.00661, 2020.\\nMaxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without\\nsupervision.arXiv preprint arXiv:2304.07193, 2023.\\nAnkur P Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan\\nDas. Totto: A controlled table-to-text generation dataset.arXiv preprint arXiv:2004.14373, 2020.\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7'}, page_content='Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language\\nsupervision. InInternational conference on machine learning, pages 8748‚Äì8763. PMLR, 2021.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a\\nlanguage model?arXiv preprint arXiv:2002.08910, 2020.\\nDustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa:\\nA benchmark for visual question answering using world knowledge. InEuropean conference on computer\\nvision, pages 146‚Äì162. Springer, 2022.\\nZhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan\\nGui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf.\\narXiv preprint arXiv:2309.14525, 2023.\\nShengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan\\nYang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A fully open, vision-centric exploration\\nof multimodal llms.arXiv preprint arXiv:2406.16860, 2024.\\nXintong Wang, Jingheng Pan, Liang Ding, and Chris Biemann. Mitigating hallucinations in large vision-language\\nmodels with instruction contrastive decoding.arXiv preprint arXiv:2403.18715, 2024.\\nSam Wiseman, Stuart M Shieber, and Alexander M Rush. Challenges in data-to-document generation.arXiv\\npreprint arXiv:1707.08052, 2017.\\nSanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie.\\nConvnext v2: Co-designing and scaling convnets with masked autoencoders. InProceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pages 16133‚Äì16142, 2023.\\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,\\nDayiheng Liu, Fei Huang, et al. Qwen2 technical report.arXiv preprint arXiv:2407.10671, 2024.\\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural\\nnetworks?Advances in neural information processing systems, 27, 2014.\\nMatthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. InComputer\\nVision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\\nPart I 13, pages 818‚Äì833. Springer, 2014.\\nBohan Zhai, Shijia Yang, Xiangchen Zhao, Chenfeng Xu, Sheng Shen, Dongdi Zhao, Kurt Keutzer, Manling Li,\\nTan Yan, and Xiangjun Fan. Halle-switch: Rethinking and controlling object existence hallucinations in large\\nvision language models for detailed caption.arXiv preprint arXiv:2310.01779, 2023a.\\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-\\ntraining. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 11975‚Äì11986,\\n2023b.\\nMuru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. How language model hallucinations\\ncan snowball.arXiv preprint arXiv:2305.13534, 2023.\\nZhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: En-\\nhancing lvlms through hallucination-aware direct preference optimization.arXiv preprint arXiv:2311.16839,\\n2023.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\\nLi, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena.Advances in\\nNeural Information Processing Systems, 36:46595‚Äì46623, 2023.\\nYiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and\\nHuaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models.arXiv preprint\\narXiv:2310.00754, 2023.\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8'}, page_content='A Appendix\\nA.1 Related Work\\nHallucination in LLMsThe causes of hallucinations in LLMs are multifaceted, with studies linking\\nthem to both training data quality Lin et al. [2021] and architectural limitations Li et al. [2023b],\\nLiu et al. [2024a]. For instance, heuristically paired data during dataset construction can sometimes\\nresult in inconsistent or unsupported outputs, exacerbating the hallucination issue Lebret et al. [2016],\\nWiseman et al. [2017]. Additionally, limited diversity in training data, such as repetitive patterns Lee\\net al. [2021], can bias model outputs and increase the likelihood of hallucinations. Another significant\\ncause for hallucination lies within the model architecture, where issues in representation learning and\\ntoken embedding processing can distort models‚Äô understanding and amplify hallucinations Parikh et al.\\n[2020]. Furthermore, models often prioritize memorized parametric knowledge over real-time input\\ndue to reliance on information encoded during training, further compounding the problem Roberts\\net al. [2020], Longpre et al. [2021]. Decoding strategies also play a role, with some methods\\nintroducing early-generation errors that accumulate rather than being corrected Zhang et al. [2023],\\nLee et al. [2022], Bengio et al. [2015].\\nHallucination in MLLMsThe causes of hallucinations in MLLMs are more complex than in\\nLLMs due to the integration of visual inputs. Specifically, limitations in the vision encoder, such\\nas low resolution or a bias toward salient objects, have been identified as major contributors to\\nhallucinations Zhai et al. [2023a], Li et al. [2024], Jain et al. [2024]. Additionally, the alignment\\nprocess between visual and textual representations often fails to accurately synchronize these inputs,\\nfurther exacerbating hallucination errors Jiang et al. [2024], Chen et al. [2024]. Furthermore, when\\nvisual input is incorporated into the model‚Äôs self-attention mechanism, it often receives insufficient\\nfocus, causing the model to rely more on pre-trained knowledge within the LLM component than on\\nthe actual visual content Favero et al. [2024], Leng et al. [2024].\\nRecent efforts to reduce hallucinations have employed various strategies, including the development\\nof fine-grained datasets for improved training Gunjal et al. [2024], Liu et al. [2023], enhancements to\\nthe vision encoder Jain et al. [2024], better alignment mechanisms Jiang et al. [2024], and optimized\\ndecoding strategies Huang et al. [2024], Leng et al. [2024], Wang et al. [2024]. Post-processing\\nmethods have also been utilized to address hallucinations after generation Zhou et al. [2023], and\\nreinforcement learning approaches have been explored to align models more closely with human\\npreferences, improving the accuracy of generated outputs Zhao et al. [2023], Sun et al. [2023].\\nThe most closely related work is Visual Contrastive Decoding (VCD) Leng et al. [2024], which\\nessentially contrasts output distributions generated from the original visual input with those generated\\nfrom input distorted by Gaussian noise. Compared to VCD, our LayerCD approach addresses the\\nproblem from a novel perspective: we leverage shallow features to filter out hallucinations. In\\nthe experiments, we demonstrate that LayerCD significantly outperforms VCD on two challenging\\nbenchmarks. From the computation perspective, LayerCD performs favorably against VCD: VCD\\nrequires two forward passes in the vision encoder (one normal input and one distorted input) while\\nLayerCD only needs one.\\nA.2 Experiments on MME\\nMMEThe MME benchmark Fu et al. [2024] provides a comprehensive toolbox to evaluate a wide\\nrange of capabilities in MLLMs, with a focus on perception and cognitive skills. This benchmark\\nincludes 14 different tasks, 10 of which assess perception-related abilities, while the remaining 4 are\\nfocused on cognitive processing. Following prior work Leng et al. [2024], we select theexistence\\nandcountsubsets to assess object-level hallucinations and thepositionandcolorsubsets to evaluate\\nhallucinations related to object attributes. Similar to POPE, MME contains binary questions that\\nrequire ‚ÄúYes‚Äù and ‚ÄúNo‚Äù responses. Model performance is quantified using a custom scoring formula,\\nwhich aggregates various accuracy metrics to provide an overall assessment. To ensure fairness, the\\nresults reported are averaged over five runs.\\nResults on MMEThe MME hallucination subsets have been widely used by the hallucination\\nresearch community. The results on these subsets are presented in Table 2 (left). Overall, LayerCD\\nperforms the best among the three decoding methods. In most settings, LayerCD outperforms the\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9'}, page_content='MME subset\\nModel Decoding Existence Count Position Color\\nLLaV A1.5\\nRegular169.67 ¬±3.71 113.67¬±7.18 117.67¬±10.73 140.67¬±6.55\\nVCD168.67 ¬±11.99 115.33¬±11.03 111.00¬±9.10 143.00¬±9.74\\nLayerCD176.00 ¬±3.74 119.00¬±13.97 133.33¬±11.01 157.00¬±5.10\\nCambrian\\nRegular98.67 ¬±8.06 85.67¬±18.15 66.33¬±7.48 74.33¬±8.34\\nVCD121.33 ¬±10.19 82.67¬±17.47 69.33¬±8.60 81.33¬±9.45\\nLayerCD125.33 ¬±10.19 88.67¬±9.80 75.68¬±8.73 103.67¬±5.81\\nMolmo\\nRegular78.33 ¬±0.00 73.33¬±0.00 53.33¬±0.00 48.33¬±0.00\\nVCD70.33 ¬±3.40 75.00¬±2.58 53.67¬±4.40 56.67¬±2.36\\nLayerCD78.33 ¬±0.00 63.33¬±0.00 56.67¬±0.00 53.33¬±0.00\\nTable 2: Results on the MME hallucination subsets. LayerCD beats the two baselines, i.e., regular\\ndecoding and VCD, in most cases except thecountandcolorsubsets when Molmo is used as the\\nbase model.\\nbaselines with significant margins. In theexistenceandpositiontasks, LayerCD‚Äôs gains are more\\nsignificant. However, when using Molmo in thecountandcolortasks, LayerCD shows inferior results\\nthan VCD. In particular, we observe that the percentage of ‚ÄúNo‚Äù answers in these two subsets is\\nexceptionally high for Molmo and LayerCD somehow exacerbates this problem, resulting in weaker\\nperformance.\\nA.3 Further Analysis\\nCombining LayerCD with Traditional Decoding StrategiesSince LayerCD is theoretically\\northogonal to the traditional decoding strategies, we combine them with LayerCD to see the effects.\\nSpecifically, we try the following decoding strategies: greedy decoding Germann [2003], beam\\nsearch Freitag and Al-Onaizan [2017], and regular sampling with top-p Holtzman et al. [2019], top-k,\\nand temperature normalization. We conduct the experiments using LLaV A 1.5 on POPE based on\\nCOCO dataset and using random sampling setting. The results are summarized in Table 3. It is\\nclear that LayerCD works well with all the traditional decoding strategies, with top-p, top-k, and\\ntemperature normalization demonstrating the strongest synergy (the gains are significant).\\nDecoding Strategy w/ LayerCD Accuracy\\nGreedy 83.19\\n‚úì85.67\\nTop P\\n(P= 0.9)\\n82.96\\n‚úì84.88\\nTop K\\n(K= 50)\\n83.02\\n‚úì85.23\\nTop K & Temperature\\n(K= 50;temperature= 0.7)\\n84.04\\n‚úì85.36\\nTop K & Temperature\\n(K= 50;temperature= 1.5)\\n83.95\\n‚úì85.02\\nBeam Search\\n(Numbeams = 3)\\n84.20\\n‚úì86.11\\nTable 3: Results of combining different decoding strategies with LayerCD. LayerCD works well with\\nall of them.\\nImpact of HyperparametersŒ± and Œ≤ Table 4 shows the results of varying Œ± and Œ≤, which are\\nLayerCD‚Äôs hyperparameters for controlling the contrastive amplification and constraint truncation,\\nrespectively. As Œ± increases from 0.2 to 1.0, the performance of LayerCD improves accordingly.\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10'}, page_content='Œ± Accuracy\\n0.2 83.7\\n0.4 84.18\\n0.6 85.33\\n0.8 82.45\\n1.0 85.67\\nŒ≤ Accuracy\\n0.001 81.96\\n0.01 83.01\\n0.1 85.67\\n0.2 84.75\\n0.5 81.96\\n0.9 80.58\\n(a) (b)\\nTable 4: Impact of hyperparametersŒ±andŒ≤. See Methodology section for their uses.\\nThe results suggest that a higher Œ± facilitates the amplification of the difference between the two\\noutput distributions derived from shallow and deep visual features, respectively, and hence leads to\\nbetter performance. For Œ≤, the results do not have a clear pattern. Though a higher Œ≤ leads to better\\nperformance, we find in practice that settingŒ≤too high may reduce output diversity.\\nw/ APCAverage\\n62.21\\n‚úì85.67\\nTable 5: Impact of the Adap-\\ntive Plausibility Constraint.\\nAPCdenotes the Adaptive\\nPlausibility Constraint.\\nImpact of Adaptive Plausibility ConstraintTable 5 presents\\nthe LayerCD results of LLaV A1.5-7B on POPE based on COCO\\ndataset and using random sampling setting, comparing performance\\nwith and without the Adaptive Plausibility Constraint. The results\\nindicate that the Adaptive Plausibility Constraint plays a crucial role\\nin enhancing LayerCD‚Äôs performance.\\nA higher Œ± amplifies the difference between the two output distri-\\nbutions derived from shallow and deep visual features and makes\\ndesirable tokens easier to be selected. Notably, because POPE re-\\nsponses are often single tokens and the highest-probability token is usually correct, a higher Œ≤ can\\nfilters out more distractor tokens and increases performance. However, setting Œ≤ too high may reduce\\noutput diversity.\\n10')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Multimodal_LLMs_Hallucination_Mitigation.pdf\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content='Mitigating Hallucination in Multimodal LLMs with\\nLayer Contrastive Decoding\\nBingkui Tong1 Jiaer Xia2 Kaiyang Zhou2\\n1Mohamed bin Zayed University of Artificial Intelligence\\n2Hong Kong Baptist University\\nAbstract\\nMultimodal Large Language Models (MLLMs) have shown impressive perception\\nand reasoning capabilities, yet they often suffer from hallucinations‚Äîgenerating\\noutputs that are linguistically coherent but inconsistent with the context of the\\ninput image, including inaccuracies in objects, attributes, and relations. To address\\nthis challenge, we propose a simple approach called Layer Contrastive Decoding\\n(LayerCD). Our design is motivated by the observation that shallow visual features\\nare much more likely than deep visual features to cause an MLLM to hallucinate\\nas they only capture biased, low-level information that is insufficient for high-level\\nreasoning. Therefore, LayerCD aims to filter out hallucinations by contrasting the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content='output distributions generated from visual features of different levels, specifically\\nthose from the shallow and deep layers of the vision encoder, respectively. We\\nconduct extensive experiments on two hallucination benchmarks and show that\\nLayerCD significantly outperforms current state-of-the-art. The code for LayerCD\\nis available at maifoundations/LayerCD.\\n1 Introduction\\nQ: What is the wall made of that the motorcycles are  \\nparked in front of ?\\nA: Wood \\nQ: How is the brick building associated with the scene?\\nA: The motorcycles are driving towards it \\nQ: How many men are lying on the motorcycles?\\nA: None\\nQ: What is the wall made of that the motorcycles are \\nparked in front of ?\\nA: Brick\\nQ: How is the brick building associated with the scene?\\nA: It is behind the motorcycles\\nQ: How many men are lying on the motorcycles?\\nA: One\\nFigure 1: Evaluation of LLaV A 1.5, a state-of-the-\\nart MLLM, using different levels of visual features.\\nThe results suggest that shallow features lead to'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content='significantly higher hallucination error rates com-\\npared to deeper features.\\nLLMs have long struggled with hallucination,\\na phenomenon where model outputs appear\\nplausible but are factually incorrect or fabri-\\ncated Maynez et al. [2020]. Unfortunately, mul-\\ntimodal LLMs (MLLMs)‚Äîwhich incorporate\\nan additional vision module to process images‚Äî\\nalso face this issue. In MLLMs, hallucination\\noccurs when the model generates responses that\\nare fluent and coherent yet misaligned with the\\nvisual input Liu et al. [2024b]. These inconsis-\\ntencies often manifest as inaccuracies in identi-\\nfying objects, attributes, or relationships, limit-\\ning the model‚Äôs ability to accurately interpret im-\\nages and posing a significant challenge for real-\\nworld deployment and practical applications.\\nWe propose Layer Contrastive Decoding (Lay-\\nerCD), a simple, inference-time method that re-\\nquires no architectural changes. Our approach is\\nmotivated by the key observation that MLLMs'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content='are more prone to hallucination when condi-\\ntioned on shallow versus deep visual features.\\nBy contrasting the output distributions from\\nMultimodal Algorithmic Reasoning Workshop at NeurIPS 2025 (MAR-NeurIPS 2025).\\narXiv:2509.25177v1  [cs.CV]  29 Sep 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}, page_content=\"‚Ä¶\\nFirst Layer\\nWhat color is \\nthe text on the \\nleft person's \\nclothes?\\nVision Encoder\\nVisual Input Textual Input\\nùê©ùêãùêöùê≤ùêûùê´ùêÇùêÉ\\nLast Layer\\nLanguage Model\\nDefault:  ['Black</s>']\\nToken: 'Black'  Score: 0.5645\\nToken: 'White'  Score: 0.3318\\nToken: 'Y'      Score: 0.0158\\nToken: 'Blue'   Score: 0.0148\\nToken: 'Gray'   Score: 0.0091\\nToken: 'Red'    Score: 0.0073\\nToken: 'Orange' Score: 0.0068\\nToken: 'Dark'   Score: 0.0054\\nToken: 'Green'  Score: 0.0054\\nToken: 'Silver' Score: 0.0047\\nFirst Layer Output:  ['Red</s>']\\nToken: 'Red'    Score: 0.1643\\nToken: 'Orange' Score: 0.1090\\nToken: 'Black'  Score: 0.0852\\nToken: 'Y'      Score: 0.0704\\nToken: 'Blue'   Score: 0.0600\\nToken: 'P'      Score: 0.0555\\nToken: 'Brown'  Score: 0.0497\\nToken: 'Mar'    Score: 0.0430\\nToken: 'Pur'    Score: 0.0386\\nToken: 'White'  Score: 0.0331\\nLayerCD Output:  ['White</s>']\\nToken: 'White'  Score: 0.5776\\nToken: 'Black'  Score: 0.4226\\nToken: '<unk>'  Score: 0.0000\\nToken: '<s>'    Score: 0.0000\\nToken: '<0x03>' Score: 0.0000\")]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "text_splitter.split_documents(docs)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content='Mitigating Hallucination in Multimodal LLMs with\\nLayer Contrastive Decoding\\nBingkui Tong1 Jiaer Xia2 Kaiyang Zhou2\\n1Mohamed bin Zayed University of Artificial Intelligence\\n2Hong Kong Baptist University\\nAbstract\\nMultimodal Large Language Models (MLLMs) have shown impressive perception\\nand reasoning capabilities, yet they often suffer from hallucinations‚Äîgenerating\\noutputs that are linguistically coherent but inconsistent with the context of the\\ninput image, including inaccuracies in objects, attributes, and relations. To address\\nthis challenge, we propose a simple approach called Layer Contrastive Decoding\\n(LayerCD). Our design is motivated by the observation that shallow visual features\\nare much more likely than deep visual features to cause an MLLM to hallucinate\\nas they only capture biased, low-level information that is insufficient for high-level\\nreasoning. Therefore, LayerCD aims to filter out hallucinations by contrasting the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content='output distributions generated from visual features of different levels, specifically\\nthose from the shallow and deep layers of the vision encoder, respectively. We\\nconduct extensive experiments on two hallucination benchmarks and show that\\nLayerCD significantly outperforms current state-of-the-art. The code for LayerCD\\nis available at maifoundations/LayerCD.\\n1 Introduction\\nQ: What is the wall made of that the motorcycles are  \\nparked in front of ?\\nA: Wood \\nQ: How is the brick building associated with the scene?\\nA: The motorcycles are driving towards it \\nQ: How many men are lying on the motorcycles?\\nA: None\\nQ: What is the wall made of that the motorcycles are \\nparked in front of ?\\nA: Brick\\nQ: How is the brick building associated with the scene?\\nA: It is behind the motorcycles\\nQ: How many men are lying on the motorcycles?\\nA: One\\nFigure 1: Evaluation of LLaV A 1.5, a state-of-the-\\nart MLLM, using different levels of visual features.\\nThe results suggest that shallow features lead to'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content='significantly higher hallucination error rates com-\\npared to deeper features.\\nLLMs have long struggled with hallucination,\\na phenomenon where model outputs appear\\nplausible but are factually incorrect or fabri-\\ncated Maynez et al. [2020]. Unfortunately, mul-\\ntimodal LLMs (MLLMs)‚Äîwhich incorporate\\nan additional vision module to process images‚Äî\\nalso face this issue. In MLLMs, hallucination\\noccurs when the model generates responses that\\nare fluent and coherent yet misaligned with the\\nvisual input Liu et al. [2024b]. These inconsis-\\ntencies often manifest as inaccuracies in identi-\\nfying objects, attributes, or relationships, limit-\\ning the model‚Äôs ability to accurately interpret im-\\nages and posing a significant challenge for real-\\nworld deployment and practical applications.\\nWe propose Layer Contrastive Decoding (Lay-\\nerCD), a simple, inference-time method that re-\\nquires no architectural changes. Our approach is\\nmotivated by the key observation that MLLMs'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content='are more prone to hallucination when condi-\\ntioned on shallow versus deep visual features.\\nBy contrasting the output distributions from\\nMultimodal Algorithmic Reasoning Workshop at NeurIPS 2025 (MAR-NeurIPS 2025).\\narXiv:2509.25177v1  [cs.CV]  29 Sep 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}, page_content=\"‚Ä¶\\nFirst Layer\\nWhat color is \\nthe text on the \\nleft person's \\nclothes?\\nVision Encoder\\nVisual Input Textual Input\\nùê©ùêãùêöùê≤ùêûùê´ùêÇùêÉ\\nLast Layer\\nLanguage Model\\nDefault:  ['Black</s>']\\nToken: 'Black'  Score: 0.5645\\nToken: 'White'  Score: 0.3318\\nToken: 'Y'      Score: 0.0158\\nToken: 'Blue'   Score: 0.0148\\nToken: 'Gray'   Score: 0.0091\\nToken: 'Red'    Score: 0.0073\\nToken: 'Orange' Score: 0.0068\\nToken: 'Dark'   Score: 0.0054\\nToken: 'Green'  Score: 0.0054\\nToken: 'Silver' Score: 0.0047\\nFirst Layer Output:  ['Red</s>']\\nToken: 'Red'    Score: 0.1643\\nToken: 'Orange' Score: 0.1090\\nToken: 'Black'  Score: 0.0852\\nToken: 'Y'      Score: 0.0704\\nToken: 'Blue'   Score: 0.0600\\nToken: 'P'      Score: 0.0555\\nToken: 'Brown'  Score: 0.0497\\nToken: 'Mar'    Score: 0.0430\\nToken: 'Pur'    Score: 0.0386\\nToken: 'White'  Score: 0.0331\\nLayerCD Output:  ['White</s>']\\nToken: 'White'  Score: 0.5776\\nToken: 'Black'  Score: 0.4226\\nToken: '<unk>'  Score: 0.0000\\nToken: '<s>'    Score: 0.0000\\nToken: '<0x03>' Score: 0.0000\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}, page_content=\"Token: '<0x04>' Score: 0.0000\\nToken: '<0x02>' Score: 0.0000\\nToken: '<0x01>' Score: 0.0000\\nToken: '</s>'   Score: 0.0000\\nToken: '<0x00>' Score: 0.0000\\nBlack\\nWhite\\nBlue\\nRed\\nOrange\\n‚Ä¶‚Ä¶\\n0.56\\n0.33\\n0.01\\n0.01\\n0.01\\nRed\\nOrange\\nBlack\\nBlue\\nWhite\\n‚Ä¶‚Ä¶\\n0.16\\n0.11\\n0.09\\n0.06\\n0.03\\nùê©\\nùê©‚Ä≤\\nWhite\\nBlack\\nRed\\nBlack\\nOrange\\n‚Ä¶‚Ä¶\\n0.58\\n0.41\\n0.00\\n0.00\\n0.00\\nFigure 2: Overview of Layer Contrastive Decoding (LayerCD). The main idea is to factor out\\nhallucinations by contrasting output distributions derived from different levels of visual features,\\nspecifically those from the shallow and deep layers of the vision encoder, respectively.\\nthese two feature levels, LayerCD effectively reduces hallucinations. A preliminary study using\\nLLaV A 1.5 Liu et al. [2024c] validates this; as shown in Fig. 1, using shallow features leads to\\nsignificantly higher hallucination error rates. This is because shallow features capture only low-level\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}, page_content='characteristics like edges and colors, which are insufficient for high-level reasoning and thus more\\nlikely to cause hallucinations Zeiler and Fergus [2014], Yosinski et al. [2014].\\nFig. 2 illustrates the LayerCD mechanism and the crucial differences in the resulting output distri-\\nbutions. While a model using deep features may produce hallucinations (e.g., ‚ÄúBlack‚Äù, Blue‚Äù), it\\nstill assigns non-trivial probability to the correct token (e.g., ‚ÄúWhite‚Äù). In contrast, conditioning\\non shallow features yields more high-confidence hallucinations while suppressing the correct token.\\nDrawing inspiration from Contrastive Decoding Li et al. [2022], which was designed to reduce repeti-\\ntion in text, LayerCD contrasts these two distributions. This readjustment cancels out hallucinations\\nand allows the correct token (‚ÄúWhite‚Äù) to emerge with the highest confidence.\\nIn summary, we make the following contributions in this paper: (1) We provide an important insight'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}, page_content='that shallow visual features are more prone to causing hallucinations in MLLMs than deep visual\\nfeatures; (2) Based on the insight, we propose a simple contrastive decoding approach that contrasts\\nvisual features of different layers to factor out hallucinations; (3) We demonstrate the effectiveness of\\nour approach on two hallucination benchmarks where our approach significantly outperforms current\\nstate-of-the-art.\\n2 Methodology\\nOur proposed Layer Contrastive Decoding (LayerCD) is an inference-time, architecture-free method\\nto mitigate hallucination. It is motivated by the observation that contrasting the outputs from shallow\\nvisual features (high confidence on hallucinations, low on correct tokens) and deep features (high\\nconfidence on both) can isolate and suppress erroneous tokens. During decoding, an MLLM takes\\nvisual features z and text input x to produce the next-token probability, conditioned on previous\\ntokensy <t:p(y t|x,z,y <t)\\n2.1 Layer Contrastive Decoding'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}, page_content='Contrastive Probability DistributionAs discussed previously, shallow visual features are more\\nprone than deep features to cause hallucinations and therefore the two output distributions derived\\nfrom these two types of features can form a contrastive pair, which allows the model to pinpoint and\\nthen cancel out hallucinations in the output. Let zs denote image features extracted from a shallow\\nlayer (e.g., the first layer) in the vision encoder, zd image features extracted from a deep layer (e.g.,\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3'}, page_content='the last layer), andf(¬∑)the LLM, the contrastive probability distribution can be formulated as\\np(y|x,z d,z s) =œÉ[(1 +Œ±)f(x,z d)‚àíŒ±f(x,z s)],(1)\\nwhere œÉ[¬∑] is the softmax function, and Œ± is a hyperparameter that controls the amplification of the\\ndifference between the two distributions. In particular, a largerŒ± indicates a stronger contrast between\\nthe two distributions. WhenŒ±=0, the formulation returns to regular decoding.\\nSince LayerCD only modifies the next-token probability distribution, it can be easily combined with\\nexisting decoding methods, such as nucleus sampling, beam search, and others. It is worth noting that\\nthe formulation in Eq. 1 does not require modifying the internal model parameters. Unless otherwise\\nspecified, we select the first and last vision encoder layer for extractingz s andz d, respectively.\\nAdaptive Plausibility ConstraintThe formulation in Eq. 1 indiscriminately penalizes all outputs'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3'}, page_content='from the shallow-feature model, an assumption that is too strong in practice. Common tokens like\\narticles (e.g., ‚Äúa‚Äù, ‚Äúthe‚Äù) are often predicted with high confidence regardless of feature depth, and\\nshould not be penalized. To prevent the generation of implausible outputs, we introduce an adaptive\\nplausibility constraint, inspired by Li et al. Li et al. [2022]. At each inference step, we dynamically\\nfilter the vocabulary by removing tokens whose confidence in the original deep-feature distribution\\nfalls below a threshold based on the maximum confidence. This ensures that LayerCD is computed\\nusing only this updated set of plausible tokens:\\nVhead(y<t) ={yt ‚àà V:p(y t|x,z d,y <t)‚â•Œ≤max\\nw\\np(w|x,z d,y <t)}, (2)\\nwhere V is the vocabulary of the MLLM andŒ≤ is a hyperparameter between 0 and 1 that controls how\\naggressively low-confidence tokens are pruned. A larger Œ≤ results in a more aggressive truncation,\\nretaining only tokens with higher confidence.1'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3'}, page_content='By combining the contrastive probability distribution with the adaptive plausibility constraint, the\\nt-th token is computed as\\np(yt|x,z d,z s,y <t)s.t.y t ‚àà Vhead(y<t). (3)\\n3 Experiments on POPE\\nPOPEWe evaluate hallucination using the POPE benchmark Li et al. [2023a], which contains\\nimages from MSCOCO Lin et al. [2014], A-OKVQA Schwenk et al. [2022], and GQA Hudson\\nand Manning [2019]. Models must answer yes/no questions probing for objects that are absent,\\ngenerated viarandom,popular(high-frequency), andadversarial(co-occurring) sampling. We report\\nthe average Accuracy, Precision, Recall, and F1 scores over five runs.\\nModels and BaselineTo evaluate the robustness and adaptability of our methods, we select three\\nstate-of-the-art MLLMs with diverse architectures: LLaV A-v1.5-7B Liu et al. [2024c], Cambrian-\\n8B Tong et al. [2024], and MoLmo-7B-D Deitke et al. [2025]. LLaV A-v1.5-7B utilizes a CLIP'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3'}, page_content='ViT-L/14@336 Radford et al. [2021] vision encoder and the Vicuna-v1.5-7B Zheng et al. [2023]\\nLLM. Cambrian-8B combines multiple vision encoders (e.g., CLIP ViT-L/14@336 Radford et al.\\n[2021], ConvNeXt-XXL@1024 Woo et al. [2023], DINOv2 Giant Oquab et al. [2023], and SigLIP\\nViT-SO400M/14@384 Zhai et al. [2023b]) with the LLaMA3-8B-Instruct Dubey et al. [2024] LLM.\\nWe chose MoLmo-7B-D, which pairs a CLIP ViT-L/336 Radford et al. [2021] encoder with the\\nQWen2-7B Yang et al. [2024] LLM, due to its superior performance over QWen-VL Bai et al. [2023]\\non 11 benchmarks.\\nFor baselines, we compare LayerCD with regular decoding and VCD Leng et al. [2024]. Please refer\\nto Appendix for discussions on the differences between LayerCD and VCD.\\nImplementation DetailsFor LayerCD, we set the contrastive amplification parameter Œ± to 1 and\\nthe plausibility constraint‚Äôs truncation parameter Œ≤ to 0.1. The shallow features zs and deep features'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3'}, page_content='zd are extracted from the first and last default layer of the vision encoder, respectively. The output\\ntokens are sampled from the post-softmax layer after the decoding strategy is applied.\\n1Similar to Eq. 1, we compute this constraint using logit value instead of post-softmax probability.\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4'}, page_content='Setting Model Decoding Accuracy Precision Recall F1 Score\\nRandom\\nLLaV A1.5\\nRegular83.21 ¬±0.49 92.21¬±0.70 72.55¬±0.63 81.20¬±0.56\\nVCD82.33 ¬±0.16 95.42¬±0.52 67.93¬±0.30 79.36¬±0.17\\nLayerCD85.77 ¬±0.25 96.39¬±0.32 74.32¬±0.59 83.93¬±0.34\\nCambrian\\nRegular62.47 ¬±0.40 81.22¬±1.58 32.45¬±0.55 46.37¬±0.56\\nVCD64.59 ¬±0.56 94.54¬±1.43 30.96¬±0.82 46.64¬±1.04\\nLayerCD75.65 ¬±0.33 97.21¬±0.22 52.81¬±0.75 68.44¬±0.60\\nMolmo\\nRegular45.67 ¬±4.08 39.87¬±8.55 18.85¬±6.24 25.54¬±7.44\\nVCD50.29 ¬±0.47 50.58¬±0.96 24.80¬±1.04 33.28¬±1.14\\nLayerCD60.79 ¬±0.05 63.49¬±0.11 50.81¬±0.56 56.45¬±0.30\\nPopular\\nLLaV A1.5\\nRegular81.83 ¬±0.47 88.99¬±0.53 72.64¬±0.66 79.99¬±0.55\\nVCD80.99 ¬±0.17 91.87¬±0.57 68.01¬±0.28 78.16¬±0.16\\nLayerCD84.25 ¬±0.25 92.60¬±0.17 74.44¬±0.59 82.53¬±0.34\\nCambrian\\nRegular61.18 ¬±0.56 76.19¬±1.84 32.56¬±0.55 45.61¬±0.64\\nVCD62.91 ¬±0.68 88.19¬±1.28 29.81¬±1.18 44.55¬±1.42\\nLayerCD73.87 ¬±0.30 91.45¬±0.71 52.68¬±0.68 66.84¬±0.50\\nMolmo\\nRegular44.78 ¬±3.23 37.52¬±7.52 16.43¬±4.45 22.83¬±5.68\\nVCD49.21 ¬±0.32 48.22¬±0.77 21.47¬±0.77 29.70¬±0.89'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4'}, page_content='LayerCD58.83 ¬±0.33 61.60¬±0.45 46.92¬±0.26 53.27¬±0.34\\nAdversarial\\nLLaV A1.5\\nRegular79.04 ¬±0.43 83.35¬±0.41 72.57¬±0.62 77.59¬±0.50\\nVCD78.32 ¬±0.35 85.72¬±0.93 67.97¬±0.33 75.82¬±0.26\\nLayerCD82.09 ¬±0.43 87.96¬±0.82 74.36¬±0.50 80.59¬±0.42\\nCambrian\\nRegular61.23 ¬±0.44 76.66¬±1.45 32.31¬±0.67 45.45¬±0.69\\nVCD62.31 ¬±0.36 84.67¬±1.71 30.12¬±1.32 44.40¬±1.29\\nLayerCD73.97 ¬±0.28 91.94¬±0.56 52.56¬±0.66 66.88¬±0.50\\nMolmo\\nRegular44.81 ¬±3.29 39.05¬±6.78 19.25¬±4.51 25.77¬±5.50\\nVCD48.43 ¬±0.35 46.87¬±0.77 23.59¬±0.96 31.38¬±1.00\\nLayerCD58.06 ¬±0.62 59.92¬±0.76 48.69¬±0.62 53.73¬±0.68\\nTable 1: Results on POPE-MSCOCO. Our LayerCD outperforms VCD and regular decoding by\\nsignificant margins. The results on POPE-A-OKVQA and POPE-GQA are provided in the supple-\\nmentary where the conclusion remains the same.\\nResults on POPETable 1 summarizes the results of applying different decoding strategies to\\ndifferent base models in the three distinct sampling settings. Overall, LayerCD achieves robust perfor-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4'}, page_content='mance across all sampling settings and with different base models. Compared with regular decoding\\nand VCD, LayerCD gains significant improvements consistently across all settings. These results\\nstrongly demonstrate the effectiveness of using shallow features to filter out object hallucinations\\n(POPE only focuses on object hallucinations). It is worth noting that all models have relatively weak\\nperformance in terms of recall. This is probably due to the training data bias that causes the model to\\nanswer ‚ÄúNo‚Äù. Nonetheless, LayerCD still demonstrates huge gains against the baselines.\\n4 Conclusion and Limitations\\nIn this work, we propose Layer Contrastive Decoding (LayerCD), a simple and effective approach\\nbuilt on the key observation that shallow visual features are significantly more prone to inducing\\nMLLM hallucinations than deep ones. By leveraging the contrast between these feature levels,\\nLayerCD effectively pinpoints and cancels out erroneous content.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4'}, page_content='A primary limitation of our approach is the doubled computational cost, as it requires separate forward\\npasses for shallow and deep features‚Äîa drawback inherited from the original contrastive decoding\\ndesign. This presents a challenge for extremely large models, and we leave the development of more\\nefficient contrastive frameworks for future work.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5'}, page_content='5 Acknowledgements\\nThis research is supported by Hong Kong Research Grants Council Early Career Scheme (No.\\n22200824).\\nReferences\\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and\\nJingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and\\nbeyond.arXiv preprint arXiv:2308.12966, 1(2):3, 2023.\\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction\\nwith recurrent neural networks.Advances in neural information processing systems, 28, 2015.\\nZhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou\\nZhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic\\ntasks. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\\n24185‚Äì24198, 2024.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5'}, page_content='24185‚Äì24198, 2024.\\nMatt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi,\\nNiklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong\\nNgo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen\\nBastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg,\\nMichael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta,\\nKuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa\\nSchoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick,\\nAli Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art\\nvision-language models. pages 91‚Äì104, June 2025.\\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5'}, page_content='Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.arXiv preprint\\narXiv:2407.21783, 2024.\\nAlessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro\\nAchille, Ashwin Swaminathan, and Stefano Soatto. Multi-modal hallucination control by visual information\\ngrounding. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\\n14303‚Äì14312, 2024.\\nMarkus Freitag and Yaser Al-Onaizan. Beam search strategies for neural machine translation.arXiv preprint\\narXiv:1702.01806, 2017.\\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,\\nKe Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for\\nmultimodal large language models, 2024. URLhttps://arxiv.org/abs/2306.13394.\\nUlrich Germann. Greedy decoding for statistical machine translation in almost linear time. InProceedings of'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5'}, page_content='the 2003 Human Language Technology Conference of the North American Chapter of the Association for\\nComputational Linguistics, pages 72‚Äì79, 2003.\\nAnisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language\\nmodels. InProceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 18135‚Äì18143,\\n2024.\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration.\\narXiv preprint arXiv:1904.09751, 2019.\\nQidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and\\nNenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty\\nand retrospection-allocation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 13418‚Äì13427, 2024.\\nDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5'}, page_content='compositional question answering. InProceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 6700‚Äì6709, 2019.\\nJitesh Jain, Jianwei Yang, and Humphrey Shi. Vcoder: Versatile vision encoders for multimodal large language\\nmodels. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\\n27992‚Äì28002, 2024.\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6'}, page_content='Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang,\\nand Shikun Zhang. Hallucination augmented contrastive learning for multimodal large language model. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27036‚Äì27046,\\n2024.\\nR√©mi Lebret, David Grangier, and Michael Auli. Neural text generation from structured data with application to\\nthe biography domain.arXiv preprint arXiv:1603.07771, 2016.\\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and\\nNicholas Carlini. Deduplicating training data makes language models better.arXiv preprint arXiv:2107.06499,\\n2021.\\nNayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, and Bryan Catanzaro.\\nFactuality enhanced language models for open-ended text generation.Advances in Neural Information\\nProcessing Systems, 35:34586‚Äì34599, 2022.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6'}, page_content='Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating\\nobject hallucinations in large vision-language models through visual contrastive decoding. InProceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13872‚Äì13882, 2024.\\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer,\\nand Mike Lewis. Contrastive decoding: Open-ended text generation as optimization.arXiv preprint\\narXiv:2210.15097, 2022.\\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucina-\\ntion in large vision-language models.arXiv preprint arXiv:2305.10355, 2023a.\\nZhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai.\\nMonkey: Image resolution and text label are important things for large multi-modal models. InProceedings'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6'}, page_content='of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26763‚Äì26773, 2024.\\nZuchao Li, Shitou Zhang, Hai Zhao, Yifei Yang, and Dongjie Yang. Batgpt: A bidirectional autoregessive talker\\nfrom generative pre-trained transformer.arXiv preprint arXiv:2307.00360, 2023b.\\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods.\\narXiv preprint arXiv:2109.07958, 2021.\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and\\nC Lawrence Zitnick. Microsoft coco: Common objects in context. InComputer Vision‚ÄìECCV 2014: 13th\\nEuropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740‚Äì755.\\nSpringer, 2014.\\nBingbin Liu, Jordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Exposing attention glitches\\nwith flip-flop language modeling.Advances in Neural Information Processing Systems, 36, 2024a.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6'}, page_content='Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in\\nlarge multi-modal models via robust instruction tuning. InThe Twelfth International Conference on Learning\\nRepresentations, 2023.\\nHanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and\\nWei Peng. A survey on hallucination in large vision-language models.arXiv preprint arXiv:2402.00253,\\n2024b.\\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26296‚Äì26306,\\n2024c.\\nShayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based\\nknowledge conflicts in question answering.arXiv preprint arXiv:2109.05052, 2021.\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6'}, page_content='abstractive summarization.arXiv preprint arXiv:2005.00661, 2020.\\nMaxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without\\nsupervision.arXiv preprint arXiv:2304.07193, 2023.\\nAnkur P Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan\\nDas. Totto: A controlled table-to-text generation dataset.arXiv preprint arXiv:2004.14373, 2020.\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7'}, page_content='Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language\\nsupervision. InInternational conference on machine learning, pages 8748‚Äì8763. PMLR, 2021.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a\\nlanguage model?arXiv preprint arXiv:2002.08910, 2020.\\nDustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa:\\nA benchmark for visual question answering using world knowledge. InEuropean conference on computer\\nvision, pages 146‚Äì162. Springer, 2022.\\nZhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan\\nGui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf.\\narXiv preprint arXiv:2309.14525, 2023.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7'}, page_content='Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan\\nYang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A fully open, vision-centric exploration\\nof multimodal llms.arXiv preprint arXiv:2406.16860, 2024.\\nXintong Wang, Jingheng Pan, Liang Ding, and Chris Biemann. Mitigating hallucinations in large vision-language\\nmodels with instruction contrastive decoding.arXiv preprint arXiv:2403.18715, 2024.\\nSam Wiseman, Stuart M Shieber, and Alexander M Rush. Challenges in data-to-document generation.arXiv\\npreprint arXiv:1707.08052, 2017.\\nSanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie.\\nConvnext v2: Co-designing and scaling convnets with masked autoencoders. InProceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pages 16133‚Äì16142, 2023.\\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7'}, page_content='Dayiheng Liu, Fei Huang, et al. Qwen2 technical report.arXiv preprint arXiv:2407.10671, 2024.\\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural\\nnetworks?Advances in neural information processing systems, 27, 2014.\\nMatthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. InComputer\\nVision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\\nPart I 13, pages 818‚Äì833. Springer, 2014.\\nBohan Zhai, Shijia Yang, Xiangchen Zhao, Chenfeng Xu, Sheng Shen, Dongdi Zhao, Kurt Keutzer, Manling Li,\\nTan Yan, and Xiangjun Fan. Halle-switch: Rethinking and controlling object existence hallucinations in large\\nvision language models for detailed caption.arXiv preprint arXiv:2310.01779, 2023a.\\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7'}, page_content='training. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 11975‚Äì11986,\\n2023b.\\nMuru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. How language model hallucinations\\ncan snowball.arXiv preprint arXiv:2305.13534, 2023.\\nZhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: En-\\nhancing lvlms through hallucination-aware direct preference optimization.arXiv preprint arXiv:2311.16839,\\n2023.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\\nLi, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena.Advances in\\nNeural Information Processing Systems, 36:46595‚Äì46623, 2023.\\nYiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and\\nHuaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models.arXiv preprint\\narXiv:2310.00754, 2023.\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8'}, page_content='A Appendix\\nA.1 Related Work\\nHallucination in LLMsThe causes of hallucinations in LLMs are multifaceted, with studies linking\\nthem to both training data quality Lin et al. [2021] and architectural limitations Li et al. [2023b],\\nLiu et al. [2024a]. For instance, heuristically paired data during dataset construction can sometimes\\nresult in inconsistent or unsupported outputs, exacerbating the hallucination issue Lebret et al. [2016],\\nWiseman et al. [2017]. Additionally, limited diversity in training data, such as repetitive patterns Lee\\net al. [2021], can bias model outputs and increase the likelihood of hallucinations. Another significant\\ncause for hallucination lies within the model architecture, where issues in representation learning and\\ntoken embedding processing can distort models‚Äô understanding and amplify hallucinations Parikh et al.\\n[2020]. Furthermore, models often prioritize memorized parametric knowledge over real-time input'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8'}, page_content='due to reliance on information encoded during training, further compounding the problem Roberts\\net al. [2020], Longpre et al. [2021]. Decoding strategies also play a role, with some methods\\nintroducing early-generation errors that accumulate rather than being corrected Zhang et al. [2023],\\nLee et al. [2022], Bengio et al. [2015].\\nHallucination in MLLMsThe causes of hallucinations in MLLMs are more complex than in\\nLLMs due to the integration of visual inputs. Specifically, limitations in the vision encoder, such\\nas low resolution or a bias toward salient objects, have been identified as major contributors to\\nhallucinations Zhai et al. [2023a], Li et al. [2024], Jain et al. [2024]. Additionally, the alignment\\nprocess between visual and textual representations often fails to accurately synchronize these inputs,\\nfurther exacerbating hallucination errors Jiang et al. [2024], Chen et al. [2024]. Furthermore, when'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8'}, page_content='visual input is incorporated into the model‚Äôs self-attention mechanism, it often receives insufficient\\nfocus, causing the model to rely more on pre-trained knowledge within the LLM component than on\\nthe actual visual content Favero et al. [2024], Leng et al. [2024].\\nRecent efforts to reduce hallucinations have employed various strategies, including the development\\nof fine-grained datasets for improved training Gunjal et al. [2024], Liu et al. [2023], enhancements to\\nthe vision encoder Jain et al. [2024], better alignment mechanisms Jiang et al. [2024], and optimized\\ndecoding strategies Huang et al. [2024], Leng et al. [2024], Wang et al. [2024]. Post-processing\\nmethods have also been utilized to address hallucinations after generation Zhou et al. [2023], and\\nreinforcement learning approaches have been explored to align models more closely with human\\npreferences, improving the accuracy of generated outputs Zhao et al. [2023], Sun et al. [2023].'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8'}, page_content='The most closely related work is Visual Contrastive Decoding (VCD) Leng et al. [2024], which\\nessentially contrasts output distributions generated from the original visual input with those generated\\nfrom input distorted by Gaussian noise. Compared to VCD, our LayerCD approach addresses the\\nproblem from a novel perspective: we leverage shallow features to filter out hallucinations. In\\nthe experiments, we demonstrate that LayerCD significantly outperforms VCD on two challenging\\nbenchmarks. From the computation perspective, LayerCD performs favorably against VCD: VCD\\nrequires two forward passes in the vision encoder (one normal input and one distorted input) while\\nLayerCD only needs one.\\nA.2 Experiments on MME\\nMMEThe MME benchmark Fu et al. [2024] provides a comprehensive toolbox to evaluate a wide\\nrange of capabilities in MLLMs, with a focus on perception and cognitive skills. This benchmark'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8'}, page_content='includes 14 different tasks, 10 of which assess perception-related abilities, while the remaining 4 are\\nfocused on cognitive processing. Following prior work Leng et al. [2024], we select theexistence\\nandcountsubsets to assess object-level hallucinations and thepositionandcolorsubsets to evaluate\\nhallucinations related to object attributes. Similar to POPE, MME contains binary questions that\\nrequire ‚ÄúYes‚Äù and ‚ÄúNo‚Äù responses. Model performance is quantified using a custom scoring formula,\\nwhich aggregates various accuracy metrics to provide an overall assessment. To ensure fairness, the\\nresults reported are averaged over five runs.\\nResults on MMEThe MME hallucination subsets have been widely used by the hallucination\\nresearch community. The results on these subsets are presented in Table 2 (left). Overall, LayerCD\\nperforms the best among the three decoding methods. In most settings, LayerCD outperforms the\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9'}, page_content='MME subset\\nModel Decoding Existence Count Position Color\\nLLaV A1.5\\nRegular169.67 ¬±3.71 113.67¬±7.18 117.67¬±10.73 140.67¬±6.55\\nVCD168.67 ¬±11.99 115.33¬±11.03 111.00¬±9.10 143.00¬±9.74\\nLayerCD176.00 ¬±3.74 119.00¬±13.97 133.33¬±11.01 157.00¬±5.10\\nCambrian\\nRegular98.67 ¬±8.06 85.67¬±18.15 66.33¬±7.48 74.33¬±8.34\\nVCD121.33 ¬±10.19 82.67¬±17.47 69.33¬±8.60 81.33¬±9.45\\nLayerCD125.33 ¬±10.19 88.67¬±9.80 75.68¬±8.73 103.67¬±5.81\\nMolmo\\nRegular78.33 ¬±0.00 73.33¬±0.00 53.33¬±0.00 48.33¬±0.00\\nVCD70.33 ¬±3.40 75.00¬±2.58 53.67¬±4.40 56.67¬±2.36\\nLayerCD78.33 ¬±0.00 63.33¬±0.00 56.67¬±0.00 53.33¬±0.00\\nTable 2: Results on the MME hallucination subsets. LayerCD beats the two baselines, i.e., regular\\ndecoding and VCD, in most cases except thecountandcolorsubsets when Molmo is used as the\\nbase model.\\nbaselines with significant margins. In theexistenceandpositiontasks, LayerCD‚Äôs gains are more\\nsignificant. However, when using Molmo in thecountandcolortasks, LayerCD shows inferior results'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9'}, page_content='than VCD. In particular, we observe that the percentage of ‚ÄúNo‚Äù answers in these two subsets is\\nexceptionally high for Molmo and LayerCD somehow exacerbates this problem, resulting in weaker\\nperformance.\\nA.3 Further Analysis\\nCombining LayerCD with Traditional Decoding StrategiesSince LayerCD is theoretically\\northogonal to the traditional decoding strategies, we combine them with LayerCD to see the effects.\\nSpecifically, we try the following decoding strategies: greedy decoding Germann [2003], beam\\nsearch Freitag and Al-Onaizan [2017], and regular sampling with top-p Holtzman et al. [2019], top-k,\\nand temperature normalization. We conduct the experiments using LLaV A 1.5 on POPE based on\\nCOCO dataset and using random sampling setting. The results are summarized in Table 3. It is\\nclear that LayerCD works well with all the traditional decoding strategies, with top-p, top-k, and\\ntemperature normalization demonstrating the strongest synergy (the gains are significant).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9'}, page_content='Decoding Strategy w/ LayerCD Accuracy\\nGreedy 83.19\\n‚úì85.67\\nTop P\\n(P= 0.9)\\n82.96\\n‚úì84.88\\nTop K\\n(K= 50)\\n83.02\\n‚úì85.23\\nTop K & Temperature\\n(K= 50;temperature= 0.7)\\n84.04\\n‚úì85.36\\nTop K & Temperature\\n(K= 50;temperature= 1.5)\\n83.95\\n‚úì85.02\\nBeam Search\\n(Numbeams = 3)\\n84.20\\n‚úì86.11\\nTable 3: Results of combining different decoding strategies with LayerCD. LayerCD works well with\\nall of them.\\nImpact of HyperparametersŒ± and Œ≤ Table 4 shows the results of varying Œ± and Œ≤, which are\\nLayerCD‚Äôs hyperparameters for controlling the contrastive amplification and constraint truncation,\\nrespectively. As Œ± increases from 0.2 to 1.0, the performance of LayerCD improves accordingly.\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10'}, page_content='Œ± Accuracy\\n0.2 83.7\\n0.4 84.18\\n0.6 85.33\\n0.8 82.45\\n1.0 85.67\\nŒ≤ Accuracy\\n0.001 81.96\\n0.01 83.01\\n0.1 85.67\\n0.2 84.75\\n0.5 81.96\\n0.9 80.58\\n(a) (b)\\nTable 4: Impact of hyperparametersŒ±andŒ≤. See Methodology section for their uses.\\nThe results suggest that a higher Œ± facilitates the amplification of the difference between the two\\noutput distributions derived from shallow and deep visual features, respectively, and hence leads to\\nbetter performance. For Œ≤, the results do not have a clear pattern. Though a higher Œ≤ leads to better\\nperformance, we find in practice that settingŒ≤too high may reduce output diversity.\\nw/ APCAverage\\n62.21\\n‚úì85.67\\nTable 5: Impact of the Adap-\\ntive Plausibility Constraint.\\nAPCdenotes the Adaptive\\nPlausibility Constraint.\\nImpact of Adaptive Plausibility ConstraintTable 5 presents\\nthe LayerCD results of LLaV A1.5-7B on POPE based on COCO\\ndataset and using random sampling setting, comparing performance\\nwith and without the Adaptive Plausibility Constraint. The results'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Bingkui Tong; Jiaer Xia; Kaiyang Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2509.25177', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.25177v1', 'source': 'Multimodal_LLMs_Hallucination_Mitigation.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10'}, page_content='indicate that the Adaptive Plausibility Constraint plays a crucial role\\nin enhancing LayerCD‚Äôs performance.\\nA higher Œ± amplifies the difference between the two output distri-\\nbutions derived from shallow and deep visual features and makes\\ndesirable tokens easier to be selected. Notably, because POPE re-\\nsponses are often single tokens and the highest-probability token is usually correct, a higher Œ≤ can\\nfilters out more distractor tokens and increases performance. However, setting Œ≤ too high may reduce\\noutput diversity.\\n10')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db=FAISS.from_documents(documents[:30],OllamaEmbeddings(model=\"llama3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1132b8f80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language\\nsupervision. InInternational conference on machine learning, pages 8748‚Äì8763. PMLR, 2021.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a\\nlanguage model?arXiv preprint arXiv:2002.08910, 2020.\\nDustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa:\\nA benchmark for visual question answering using world knowledge. InEuropean conference on computer\\nvision, pages 146‚Äì162. Springer, 2022.\\nZhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan\\nGui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf.\\narXiv preprint arXiv:2309.14525, 2023.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"Impact of hyperparameters Œ± and Œ≤.\"\n",
    "result=db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mq/mnq950m90kg0lrbjk4rw6mc40000gn/T/ipykernel_32442/2738016233.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm=Ollama(model=\"llama3\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "## Load Ollama LAMA2 LLM model\n",
    "llm=Ollama(model=\"llama3\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. If you don't know the answer, just say that you don't know. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain Introduction\n",
    "## Create Stuff Docment Chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x1132b8f80>, search_kwargs={})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrievers: A retriever is an interface that returns documents given\n",
    " an unstructured query. It is more general than a vector store.\n",
    " A retriever does not need to be able to store documents, only to \n",
    " return (or retrieve) them. Vector stores can be used as the backbone\n",
    " of a retriever, but there are other types of retrievers as well. \n",
    " https://python.langchain.com/docs/modules/data_connection/retrievers/   \n",
    "\"\"\"\n",
    "\n",
    "retriever=db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieval chain:This chain takes in a user inquiry, which is then\n",
    "passed to the retriever to fetch relevant documents. Those documents \n",
    "(and original inputs) are then passed to an LLM to generate a response\n",
    "https://python.langchain.com/docs/modules/chains/\n",
    "\"\"\"\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=retrieval_chain.invoke({\"input\":\"Impact of hyperparameters Œ±and Œ≤.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know. The provided context does not mention hyperparameters Œ±and Œ≤ or their impact. It appears to be a collection of research papers on various topics in computer vision and natural language processing.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
