# ğŸ¦™ LangChain + Ollama + Streamlit Demo

This project demonstrates how to run LLaMA2 locally using [Ollama](https://ollama.com/) and interact with it via a simple [Streamlit](https://streamlit.io/) interface powered by [LangChain](https://www.langchain.com/). Itâ€™s designed for developers who want a lightweight, local LLM experience with prompt chaining and UI integration.

---

## ğŸ“¦ Features

- ğŸ”— LangChain prompt orchestration and output parsing
- ğŸ§  Local LLaMA2 model served via Ollama
- ğŸ–¥ï¸ Interactive Streamlit frontend
- ğŸ” Optional LangChain tracing for observability
- ğŸ” Environment-based configuration using `.env`

---

## ğŸ§° Requirements

- Python 3.10+
- [Ollama installed locally](https://ollama.com/download)
- LLaMA2 model pulled via Ollama:
  ```bash
  ollama pull llama3
